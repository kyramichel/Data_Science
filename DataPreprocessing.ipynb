{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for building good machine learning models\n",
    "\n",
    "\n",
    "We explore a technique for feature selection that helps reduce the dimensionality of the dataset.\n",
    "\n",
    "\n",
    "Our data consists of 178 samples of wine that are the result of a chemical analysis of 3 types of wine.\n",
    "\n",
    "There are 13 features variables included in the dataset that describe their different chemical properties found in the wine samples:\n",
    "\n",
    "\n",
    "#### Data features are (in order):\n",
    "\n",
    " - Alcohol\n",
    " \n",
    " - Malic acid\n",
    "\n",
    " - Ash\n",
    "\n",
    " - Alcalinity of ash\n",
    "\n",
    " - Magnesium\n",
    "\n",
    " - Total phenols\n",
    "\n",
    " - Flavanoids\n",
    "\n",
    " - Nonflavanoid phenols\n",
    "\n",
    " - Proanthocyanins\n",
    "\n",
    " - Color intensity\n",
    "  \n",
    " - Hue\n",
    "   \n",
    " - OD280/OD315 of diluted wines\n",
    "   \n",
    " - Proline\n",
    "\n",
    "\n",
    "\n",
    "#### There are 3 types of wine:\n",
    "\n",
    "- Class label variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Data Source is available at:*\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class label</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class label  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0            1    14.23        1.71  2.43               15.6        127   \n",
       "1            1    13.20        1.78  2.14               11.2        100   \n",
       "2            1    13.16        2.36  2.67               18.6        101   \n",
       "3            1    14.37        1.95  2.50               16.8        113   \n",
       "4            1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data:\n",
    "\n",
    "df = pd.read_csv('C:/Users/uknow/Desktop/wine.data', header=None)\n",
    "df.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash','Alcalinity of ash', 'Magnesium','Total phenols', 'Flavanoids', 'Nonflavanoid phenols','Proanthocyanins',\n",
    "'Color intensity', 'Hue','OD280/OD315 of diluted wines','Proline']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class label                       int64\n",
       "Alcohol                         float64\n",
       "Malic acid                      float64\n",
       "Ash                             float64\n",
       "Alcalinity of ash               float64\n",
       "Magnesium                         int64\n",
       "Total phenols                   float64\n",
       "Flavanoids                      float64\n",
       "Nonflavanoid phenols            float64\n",
       "Proanthocyanins                 float64\n",
       "Color intensity                 float64\n",
       "Hue                             float64\n",
       "OD280/OD315 of diluted wines    float64\n",
       "Proline                           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class label                     0\n",
       "Alcohol                         0\n",
       "Malic acid                      0\n",
       "Ash                             0\n",
       "Alcalinity of ash               0\n",
       "Magnesium                       0\n",
       "Total phenols                   0\n",
       "Flavanoids                      0\n",
       "Nonflavanoid phenols            0\n",
       "Proanthocyanins                 0\n",
       "Color intensity                 0\n",
       "Hue                             0\n",
       "OD280/OD315 of diluted wines    0\n",
       "Proline                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the number of missing values per column\n",
    "\n",
    "df.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 3 class labels\n",
    "\n",
    "df[\"Class label\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 178 samples samples that belong to one of three different classes, 1, 2, and 3 which refer to the three different types of wine.\n",
    "\n",
    "A convenient way to randomly partition this dataset into testing and training subsets is using train_test_split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.iloc[:,1:].values, df.iloc[:,0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 13) (124,)\n",
      "(54, 13) (54,)\n"
     ]
    }
   ],
   "source": [
    "# We used 70%/30 % train/test split\n",
    "# but for large datasets 90/10 is common and more appropriate\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is an important part of data preprocessing \n",
    "\n",
    "A commonly used technique is standardization, a transformation that:\n",
    "\n",
    " - centers the feature columns at mean 0 with variance 1 so that the feature columns take the form of a normal distribution, which makes it easier to learn the weights\n",
    "\n",
    "- maintains useful information about outliers and makes a machine algorithm less sensitive to them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the StandardScaler() on the X_train \n",
    "# and use those parameters to transform the test set \n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether training data is helpful at all. \n",
    "\n",
    "In practice it can happen to have a high-dimensional dataset with many features that are irrelevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting meaningful features using L1 regularization.\n",
    "\n",
    "L1 regularization is commonly used technique because it yields sparse feature vectors; most feature weights will be zero. \n",
    "\n",
    "It is especially useful when there are more irrelevant dimensions than samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression(penalty=\"l1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the L1 regularized logistic regression to the standardized training data to produce the following sparse solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=0.1, penalty=\"l1\")\n",
    "logreg.fit(X_train_std,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both training and test accuracies are 98%. \n",
    "\n",
    "Therefore, no indicator for overfitting of our model:\n",
    "\n",
    "  - Oerfitting is when the model fits the parameters closely to the particular observations in the training dataset but does not generalize well to test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9838709677419355 0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "print(logreg.score(X_train_std, y_train), logreg.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class classification with LogisticRegression that uses the OvR approach by default\n",
    "\n",
    "\n",
    "Interpretation of the intercepts and coefficients:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.38379071, -0.15807535, -0.7004143 ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept is an array with three values:\n",
    " - the intercept that belongs to the model that fits class 1 (vs class 2 & 3)\n",
    " - the intercept of the model that fits class 2 (vs class 1 & 3)\n",
    " - the intercept of the model that fits class 3 (vs class 1 & 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28009078,  0.        ,  0.        , -0.02796229,  0.        ,\n",
       "         0.        ,  0.70997012,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.23643373],\n",
       "       [-0.64401394, -0.06873457, -0.05721483,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        , -0.92680614,\n",
       "         0.0602196 ,  0.        , -0.37108924],\n",
       "       [ 0.        ,  0.06158012,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -0.63618518,  0.        ,  0.        ,  0.49819186,\n",
       "        -0.35806977, -0.57111414,  0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array of coefficients has 3 rows - one 13-dimensional weight vector for each class label 1,2,3.\n",
    "\n",
    " - In calculating the net input in computing the probabilities, each row consisting of 13 features is multiplied by the 13D weight vector corresponding to its class label.\n",
    "\n",
    " - The coefficient matrix has only a few non-zero entries. A sparse feature vectors means many feature weights are zero.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The logistic equation: \n",
    "\n",
    " - logit(p) = log(p/1-p) = z \n",
    "\n",
    "expresses the linear relationship of the log-odds with the feature values x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = X_train.dot(logreg.coef_.T) + logreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+000, 4.78009742e-173, 3.89078776e-002],\n",
       "       [1.00000000e+000, 1.20220842e-055, 6.87289163e-002],\n",
       "       [1.00000000e+000, 2.37413246e-143, 9.57038472e-001],\n",
       "       [1.00000000e+000, 5.39277437e-140, 4.96994712e-001],\n",
       "       [1.00000000e+000, 3.48396791e-072, 4.58815384e-002],\n",
       "       [1.00000000e+000, 1.23029714e-098, 5.94495367e-001],\n",
       "       [1.00000000e+000, 8.10677242e-086, 9.14764121e-001],\n",
       "       [1.00000000e+000, 2.82178712e-213, 1.22700553e-001],\n",
       "       [1.00000000e+000, 1.58489763e-089, 3.70132933e-001],\n",
       "       [1.00000000e+000, 2.57116268e-111, 6.43633231e-001],\n",
       "       [1.00000000e+000, 1.95077697e-074, 5.92689655e-002],\n",
       "       [1.00000000e+000, 8.04663750e-066, 2.58844213e-001],\n",
       "       [1.00000000e+000, 1.79208212e-115, 6.11642895e-001],\n",
       "       [1.00000000e+000, 9.85186313e-108, 1.52587659e-001],\n",
       "       [1.00000000e+000, 7.54279876e-198, 1.22836509e-001],\n",
       "       [1.00000000e+000, 7.04753509e-089, 1.90142979e-001],\n",
       "       [1.00000000e+000, 2.08243813e-156, 6.45827608e-002],\n",
       "       [1.00000000e+000, 6.15382457e-106, 1.70075488e-001],\n",
       "       [1.00000000e+000, 5.35638389e-116, 1.23106516e-001],\n",
       "       [1.00000000e+000, 1.69711736e-250, 1.11753685e-001],\n",
       "       [1.00000000e+000, 8.81125450e-115, 3.35590863e-002],\n",
       "       [1.00000000e+000, 1.33346617e-164, 3.78631328e-002],\n",
       "       [1.00000000e+000, 3.27819406e-084, 9.48796713e-001],\n",
       "       [1.00000000e+000, 2.57431570e-109, 9.65633171e-001],\n",
       "       [1.00000000e+000, 7.05524238e-096, 7.30951380e-002],\n",
       "       [1.00000000e+000, 2.68785047e-066, 4.48100914e-002],\n",
       "       [1.00000000e+000, 9.92721053e-085, 5.22381570e-002],\n",
       "       [1.00000000e+000, 5.13219755e-121, 9.55533489e-001],\n",
       "       [1.00000000e+000, 9.59098624e-089, 6.53176874e-001],\n",
       "       [1.00000000e+000, 9.41830598e-193, 2.40451616e-001],\n",
       "       [1.00000000e+000, 1.06823506e-124, 8.90320082e-001],\n",
       "       [1.00000000e+000, 1.14074182e-105, 4.83558310e-002],\n",
       "       [1.00000000e+000, 1.14647624e-062, 1.14298463e-001],\n",
       "       [1.00000000e+000, 4.17076877e-064, 5.42820882e-002],\n",
       "       [1.00000000e+000, 1.18685044e-066, 3.50562580e-002],\n",
       "       [1.00000000e+000, 4.06739060e-121, 9.68037699e-002],\n",
       "       [1.00000000e+000, 2.02884879e-211, 3.58113698e-001],\n",
       "       [1.00000000e+000, 3.70232268e-178, 2.14402044e-001],\n",
       "       [1.00000000e+000, 5.72055208e-078, 5.65643700e-002],\n",
       "       [1.00000000e+000, 1.65086741e-192, 9.46704780e-002],\n",
       "       [1.00000000e+000, 5.42540433e-122, 5.53644782e-001],\n",
       "       [1.00000000e+000, 1.24207668e-169, 6.81671072e-002],\n",
       "       [1.00000000e+000, 2.86045181e-091, 7.83868445e-001],\n",
       "       [1.00000000e+000, 8.74076988e-133, 9.02085468e-002],\n",
       "       [1.00000000e+000, 9.35354618e-219, 2.87698009e-001],\n",
       "       [1.00000000e+000, 1.55753137e-126, 1.39269821e-001],\n",
       "       [1.00000000e+000, 2.28870465e-199, 1.56670666e-001],\n",
       "       [1.00000000e+000, 5.34881100e-120, 7.91419507e-002],\n",
       "       [1.00000000e+000, 1.66607060e-069, 1.84337605e-001],\n",
       "       [1.00000000e+000, 1.42894611e-246, 2.32721635e-001],\n",
       "       [1.00000000e+000, 1.01834913e-143, 9.25505157e-001],\n",
       "       [1.00000000e+000, 2.05333419e-084, 1.46102905e-001],\n",
       "       [1.00000000e+000, 6.37398237e-215, 1.21025071e-001],\n",
       "       [1.00000000e+000, 9.83520604e-127, 2.04600862e-001],\n",
       "       [1.00000000e+000, 1.23486127e-074, 2.05390881e-001],\n",
       "       [1.00000000e+000, 3.29529632e-089, 7.08306805e-002],\n",
       "       [1.00000000e+000, 1.18604275e-107, 4.06417258e-001],\n",
       "       [1.00000000e+000, 1.03812318e-219, 1.64119805e-001],\n",
       "       [1.00000000e+000, 1.61142785e-089, 6.19950999e-001],\n",
       "       [1.00000000e+000, 6.36565352e-076, 7.61592225e-001],\n",
       "       [1.00000000e+000, 3.08587490e-257, 2.12895308e-001],\n",
       "       [1.00000000e+000, 9.10818619e-179, 8.76080448e-002],\n",
       "       [1.00000000e+000, 3.35253615e-078, 1.82635194e-001],\n",
       "       [1.00000000e+000, 1.17973432e-117, 9.59020611e-001],\n",
       "       [1.00000000e+000, 1.45525941e-178, 8.37898582e-002],\n",
       "       [1.00000000e+000, 4.08290723e-213, 1.55285949e-001],\n",
       "       [1.00000000e+000, 1.08818848e-095, 4.88110685e-002],\n",
       "       [1.00000000e+000, 4.47337859e-076, 6.21652190e-002],\n",
       "       [1.00000000e+000, 5.65662488e-155, 9.12749446e-002],\n",
       "       [1.00000000e+000, 1.06179160e-211, 1.61118782e-001],\n",
       "       [1.00000000e+000, 3.24914325e-173, 1.11466558e-001],\n",
       "       [1.00000000e+000, 3.34802685e-127, 8.66760505e-001],\n",
       "       [1.00000000e+000, 1.92123682e-115, 1.46818387e-001],\n",
       "       [1.00000000e+000, 1.50480746e-175, 7.71045424e-002],\n",
       "       [1.00000000e+000, 3.83375335e-106, 5.06428092e-002],\n",
       "       [1.00000000e+000, 3.38420769e-095, 4.47908668e-001],\n",
       "       [1.00000000e+000, 1.96542651e-183, 7.64256418e-002],\n",
       "       [1.00000000e+000, 3.12423154e-056, 7.16600647e-002],\n",
       "       [1.00000000e+000, 4.92214578e-097, 6.39926753e-001],\n",
       "       [1.00000000e+000, 1.36323352e-110, 8.45976630e-001],\n",
       "       [1.00000000e+000, 1.62623504e-189, 2.05694019e-001],\n",
       "       [1.00000000e+000, 2.16350464e-170, 8.29379439e-002],\n",
       "       [1.00000000e+000, 3.35206151e-103, 5.57173971e-001],\n",
       "       [1.00000000e+000, 7.12333112e-229, 2.14545064e-001],\n",
       "       [1.00000000e+000, 1.39396765e-101, 5.01169548e-001],\n",
       "       [1.00000000e+000, 2.62005030e-060, 4.70251901e-002],\n",
       "       [1.00000000e+000, 7.92456571e-183, 1.05298864e-001],\n",
       "       [1.00000000e+000, 4.82954840e-154, 6.78373762e-002],\n",
       "       [1.00000000e+000, 1.81442629e-067, 5.83164763e-002],\n",
       "       [1.00000000e+000, 1.30529556e-251, 2.27022121e-001],\n",
       "       [1.00000000e+000, 4.89958897e-097, 9.14883254e-001],\n",
       "       [1.00000000e+000, 1.64996780e-080, 9.61656354e-002],\n",
       "       [1.00000000e+000, 2.94641484e-149, 8.35578465e-001],\n",
       "       [1.00000000e+000, 8.47327866e-166, 7.83104447e-002],\n",
       "       [1.00000000e+000, 2.85647068e-090, 4.59075694e-001],\n",
       "       [1.00000000e+000, 9.33558093e-089, 8.61002969e-001],\n",
       "       [1.00000000e+000, 2.19366637e-108, 8.48834303e-001],\n",
       "       [1.00000000e+000, 1.72880455e-173, 1.03357651e-001],\n",
       "       [1.00000000e+000, 1.05679816e-070, 3.27273334e-002],\n",
       "       [1.00000000e+000, 1.18373105e-106, 7.15924547e-002],\n",
       "       [1.00000000e+000, 3.82626537e-080, 4.66322371e-002],\n",
       "       [1.00000000e+000, 2.71412535e-070, 3.58196232e-002],\n",
       "       [1.00000000e+000, 8.31708522e-113, 8.89754582e-001],\n",
       "       [1.00000000e+000, 7.09574955e-099, 9.25512987e-001],\n",
       "       [1.00000000e+000, 3.19802600e-082, 1.36562379e-001],\n",
       "       [1.00000000e+000, 7.17097332e-086, 1.38839971e-001],\n",
       "       [1.00000000e+000, 9.21039888e-140, 6.13522423e-002],\n",
       "       [1.00000000e+000, 4.52753025e-121, 7.84053106e-002],\n",
       "       [1.00000000e+000, 4.79830055e-119, 9.49856260e-001],\n",
       "       [1.00000000e+000, 1.09458450e-109, 8.66493147e-001],\n",
       "       [1.00000000e+000, 1.81980048e-129, 1.01319463e-001],\n",
       "       [1.00000000e+000, 1.52132139e-214, 1.79869871e-001],\n",
       "       [1.00000000e+000, 4.31882394e-103, 4.62875552e-001],\n",
       "       [1.00000000e+000, 7.27924597e-115, 1.01341358e-001],\n",
       "       [1.00000000e+000, 9.46379128e-146, 2.50765653e-001],\n",
       "       [1.00000000e+000, 9.41865967e-096, 6.61873744e-002],\n",
       "       [1.00000000e+000, 2.83091672e-148, 1.20181769e-001],\n",
       "       [1.00000000e+000, 1.80536266e-130, 1.05563014e-001],\n",
       "       [1.00000000e+000, 4.61460720e-176, 1.95694423e-001],\n",
       "       [1.00000000e+000, 6.92661634e-072, 8.77302435e-002],\n",
       "       [1.00000000e+000, 2.19717085e-088, 1.23714727e-001],\n",
       "       [1.00000000e+000, 8.96944879e-061, 4.85832250e-002],\n",
       "       [1.00000000e+000, 5.81749783e-166, 1.25394222e-001],\n",
       "       [1.00000000e+000, 3.79610761e-115, 9.33558066e-001]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of sigmoid function:\n",
    "\n",
    "- sigmoid(z) = p\n",
    "\n",
    "where \n",
    "\n",
    "- p = Prob (class label: 1,2,3 | x;w) \n",
    "\n",
    "represents the conditional probability of particular sample belonging to a class label given its features x parameterized by the weights w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7036647 , 0.15793261, 0.13840269],\n",
       "       [0.10432374, 0.09117526, 0.80450099],\n",
       "       [0.22868198, 0.68435332, 0.08696469],\n",
       "       [0.69734262, 0.14113823, 0.16151914],\n",
       "       [0.18521545, 0.65297368, 0.16181087],\n",
       "       [0.43233612, 0.5411641 , 0.02649977],\n",
       "       [0.77385598, 0.12633572, 0.09980831],\n",
       "       [0.0692622 , 0.26316014, 0.66757766],\n",
       "       [0.1529807 , 0.70503119, 0.14198811],\n",
       "       [0.08484739, 0.68926641, 0.22588621],\n",
       "       [0.18618244, 0.24587795, 0.56793961],\n",
       "       [0.05360587, 0.26089837, 0.68549576],\n",
       "       [0.80977081, 0.05513756, 0.13509163],\n",
       "       [0.43028075, 0.50338817, 0.06633108],\n",
       "       [0.21673921, 0.07984007, 0.70342072],\n",
       "       [0.07746824, 0.85591237, 0.06661939],\n",
       "       [0.74577894, 0.15127576, 0.1029453 ],\n",
       "       [0.85835309, 0.02233509, 0.11931183],\n",
       "       [0.08379689, 0.43855941, 0.4776437 ],\n",
       "       [0.78851422, 0.13668751, 0.07479827],\n",
       "       [0.38351476, 0.52194816, 0.09453708],\n",
       "       [0.5626508 , 0.29694679, 0.14040241],\n",
       "       [0.47070608, 0.39473351, 0.13456041],\n",
       "       [0.24470218, 0.65819362, 0.0971042 ],\n",
       "       [0.13060054, 0.56104746, 0.30835199],\n",
       "       [0.09981509, 0.75387842, 0.1463065 ],\n",
       "       [0.18271969, 0.66281221, 0.1544681 ],\n",
       "       [0.10673572, 0.72919989, 0.16406439],\n",
       "       [0.17036879, 0.65685119, 0.17278002],\n",
       "       [0.14704811, 0.0642515 , 0.78870039],\n",
       "       [0.68880539, 0.20789377, 0.10330084],\n",
       "       [0.74659065, 0.11236365, 0.1410457 ],\n",
       "       [0.12918653, 0.62047279, 0.25034067],\n",
       "       [0.72243824, 0.1196556 , 0.15790617],\n",
       "       [0.76938419, 0.10413118, 0.12648463],\n",
       "       [0.5671936 , 0.32890503, 0.10390137],\n",
       "       [0.16155085, 0.21232249, 0.62612666],\n",
       "       [0.10139751, 0.59220186, 0.30640063],\n",
       "       [0.08422559, 0.79848638, 0.11728803],\n",
       "       [0.1891523 , 0.13222016, 0.67862754],\n",
       "       [0.54782646, 0.39892305, 0.05325049],\n",
       "       [0.76349306, 0.13014108, 0.10636586],\n",
       "       [0.0886199 , 0.82821156, 0.08316854],\n",
       "       [0.12688672, 0.67791556, 0.19519773],\n",
       "       [0.08196177, 0.73390997, 0.18412826],\n",
       "       [0.64453485, 0.24627938, 0.10918577],\n",
       "       [0.11653178, 0.25368049, 0.62978772],\n",
       "       [0.33980789, 0.55451567, 0.10567643],\n",
       "       [0.18812377, 0.18297217, 0.62890406],\n",
       "       [0.77533697, 0.07026651, 0.15439652],\n",
       "       [0.09893622, 0.34856944, 0.55249435],\n",
       "       [0.09916041, 0.26397829, 0.6368613 ],\n",
       "       [0.66031111, 0.22908418, 0.11060471],\n",
       "       [0.25147452, 0.00932261, 0.73920287]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict_proba(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict the class-membership probability of each sample via\n",
    "the predict_proba():\n",
    "\n",
    "-  Interpreting the probability values in the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7036647 , 0.15793261, 0.13840269])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict_proba(X_test_std)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " - 70.3% chance the first wine sample belongs to class 1\n",
    " \n",
    " - 15.7% chance the first wine sample belongs to class label 2 \n",
    " \n",
    " - 13.8% chance the first wine sample belongs to class label 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conclusion\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained a Logistic Regression model that is robust to potentially irrelevant features in this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - L1 regularization served as a method for the feature selection. \n",
    " - By preferring a simpler model can reduce the variance in the presence of sufficient training data to fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The problem of overfitting in the context of bias and variance. We applied another commonly used technique for avoiding overfitting: train/test split validation:\n",
    "\n",
    "- We prevent overfitting by not using all of the data\n",
    "- We have some remaining data we can use to evaluate our model.\n",
    "\n",
    "Train/test split validation may not sufficiently randomize the data. \n",
    " - In practice, we perform k-fold cross-validation to avoid the limitations in the train/test split\n",
    " \n",
    " - By varying the number k of folds we can get a sense of how this impacts the score, the variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical explaination of why L1 regularization can lead to sparse solutions\n",
    "\n",
    "\n",
    "\n",
    "https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
